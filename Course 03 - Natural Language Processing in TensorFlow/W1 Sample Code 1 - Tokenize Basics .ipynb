{"cells":[{"cell_type":"markdown","metadata":{"id":"kM_jiYzqOWb-"},"source":["<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"rL-LzAqpoGLC"},"source":["# Ungraded Lab: Tokenizer Basics\n","\n","In most Natural Language Processing tasks, the initial step in preparing your data is to extract a vocabulary of words from your *corpus* (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called *tokens* and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells."]},{"cell_type":"markdown","metadata":{"id":"-nt3uR9TPrUt"},"source":["## Generating the vocabulary\n","\n","In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method and you can get the result by looking at the `word_index` property. More frequent words have a lower index."]},{"cell_type":"markdown","source":["## Key points:\n","\n","\n","*   First we are creating an object of Tokenizer and passing a parameter.\n","*   Then, feeding training data into created object through fit_on_texts function.\n","*   In fit_on_texts the tokenizer will assign a particular number to every word to help computer recongize that word every single time.\n","*   After assigning number to each word we can get that data with help of function called word_index.\n","*   More frequence of words the lower index number they will have.\n","*   Tokenizer will ignore all punctuation and words are converted to lower case.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"pjdVOEB0XpFL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaCMcjMQifQc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680027323239,"user_tz":-300,"elapsed":375,"user":{"displayName":"Shareyar Farooqui","userId":"05587113932021885845"}},"outputId":"51aeaed6-1f13-4e59-e0ee-a9b03f82554e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Define input sentences\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat'\n","    ]\n","\n","# Initialize the Tokenizer class\n","tokenizer = Tokenizer(num_words = 100)\n","\n","# Generate indices for each word in the corpus\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the indices and print it\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"markdown","metadata":{"id":"uTPWesNaRdX2"},"source":["The `num_words` parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. You will see this in a later exercise. For now, the important thing to note is it does not affect how the `word_index` dictionary is generated. You can try passing `1` instead of `100` as shown on the next cell and you will arrive at the same `word_index`. \n","\n","Also notice that by default, all punctuation is ignored and words are converted to lower case. You can override these behaviors by modifying the `filters` and `lower` arguments of the `Tokenizer` class as described [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). You can try modifying these in the next cell below and compare the output to the one generated above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VX1A1pDNoVKm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680027440110,"user_tz":-300,"elapsed":514,"user":{"displayName":"Shareyar Farooqui","userId":"05587113932021885845"}},"outputId":"b33de6f0-25db-4eb8-c343-b9708a288731"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}],"source":["# Define input sentences\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat',\n","    'You love my dog!'\n","]\n","\n","# Initialize the Tokenizer class\n","tokenizer = Tokenizer(num_words = 1)\n","\n","# Generate indices for each word in the corpus\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the indices and print it\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"markdown","source":["## Demonstrating Parameters:\n","* filters:\ta string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n","\n","* lower:\tboolean, Whether to convert the texts to lowercase.\n","\n","* char_level:\tif True, every character will be treated as a token.\n","\n","* oov_token:\tif given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls.\n","\n","* analyzer:\tfunction Custom analyzer to split the text. The default analyzer is text_to_word_sequence"],"metadata":{"id":"ZFDGYP92cSQf"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Define input sentences\n","sentences = [\n","    'i lOve my dOG',\n","    'I, love my cat!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n","    ]\n","\n","# Initialize the Tokenizer class\n","tokenizer = Tokenizer(num_words = 100, filters='?', lower=False)\n","\n","# Generate indices for each word in the corpus\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the indices and print it\n","word_index = tokenizer.word_index\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQMSRf4Vbphd","executionInfo":{"status":"ok","timestamp":1680027912825,"user_tz":-300,"elapsed":461,"user":{"displayName":"Shareyar Farooqui","userId":"05587113932021885845"}},"outputId":"c67c4451-850c-48fb-dce5-9cd4f44827b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'my': 1, 'i': 2, 'lOve': 3, 'dOG': 4, 'I,': 5, 'love': 6, 'cat!\"#$%&()*+,-./:;<=>': 7, '@[\\\\]^_`{|}~\\t\\n': 8}\n"]}]},{"cell_type":"markdown","source":["NOTE: using `filter = '?'`, the tokenzier removed ? from text and divided string into 2 parts. The ? replaced with a space.\n","\n","NOTE: using `lower=False`, the tokenizer didnt converted uppercase into lowercase instead it left it in same condition."],"metadata":{"id":"lqcNfSWcdQ3V"}},{"cell_type":"markdown","metadata":{"id":"c9LFfwBffDaj"},"source":["That concludes this short exercise on tokenizing input texts!"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb","timestamp":1680025594561},{"file_id":"https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb","timestamp":1642431620601}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}